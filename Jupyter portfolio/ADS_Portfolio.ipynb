{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Swapping Technique with Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Face Swapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face Swapping is the technique of cropping or copying the face of one person and superimposing it on another persons face.\n",
    "Something like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![face-swap-baby-adult](images/01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we use Data Science for this instead of an image editing software?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If face swap was to be accomplished one could use any image editing software to crop the face of one person and try to superimpose it over another.<br>\n",
    "Assuming it takes 5 minutes to work on one image, consider a video of 1 minute with 24 fps (frames per second range varies from 23-30), this would make 1440 images (60 seconds x 24 fps) and the time spent to replace the face for these would be **5 days** (1440 images x 5 minutes=7200 minutes / 60 mins = 120 hours / 24 hours = 5 days) not including room for human error.\n",
    "<br><br>\n",
    "Wouldn't it be better for a software program to do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following are a few good and bad applications of this technique:\n",
    "- Fun Picture Meme: Swapping your friends face with their dog can be funny at times\n",
    "- News Videos: Fake news and people going all crazy about how someone said something\n",
    "- Deepfakes (Needless to say)\n",
    "- Movies: An actors stunt could be face swapped with a stunt double saving a lot of money for movie makers\n",
    "- Fun Video Meme: What would Chris Hemsworth's (Thor) face look like acting as Robert Downey Jr. in Iron Man\n",
    "\n",
    "Our focus is going to be the last application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Auto Encoder?\n",
    "![mushroom encoder](images/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset:\n",
    "There are two datasets for training our model, one is Chris Hemsworth’s face images & the second is Robert Downey Jr’s face images. The number of images are in the range 350 – 450. Each image is of dimension 256 x 256 and have been picked on the basis of clarity from a set of 2000 images. These images were obtained by scraping the internet. Originally the images obtained had other people in it and therefore the faces had to be extracted.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset](images/03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for extracting faces\n",
    "```python\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from lib.cli import DirectoryProcessor\n",
    "from plugins.PluginLoader import PluginLoader\n",
    "\n",
    "class ExtractTrainingData(DirectoryProcessor):\n",
    "    def create_parser(self, subparser, command, description):\n",
    "        self.parser = subparser.add_parser(\n",
    "            command,\n",
    "            help=\"Extract the faces from a pictures.\",\n",
    "            description=description)\n",
    "        \n",
    "    def process(self):\n",
    "        extractor_name = \"Align\"  # DEFINED BELOW THIS CLASS\n",
    "        extractor = PluginLoader.get_extractor(extractor_name)()\n",
    "\n",
    "        try:\n",
    "            for filename in self.read_directory():\n",
    "                image = cv2.imread(filename)\n",
    "                for idx, face in self.get_faces(image):\n",
    "                    resized_image = extractor.extract(image, face, 256)\n",
    "                    output_file = self.output_dir / Path(filename).stem\n",
    "                    cv2.imwrite(str(output_file) + str(idx) + Path(filename).suffix, resized_image)\n",
    "        except Exception as e:\n",
    "            print('Failed to extract from image: {}. Reason: {}'.format(filename, e))\n",
    "```       \n",
    "            \n",
    "### Code for Alignment            \n",
    "```python            \n",
    "import cv2\n",
    "from lib.aligner import get_align_mat\n",
    "\n",
    "class Extract(object):\n",
    "    def extract(self, image, face, size):\n",
    "        if face.landmarks == None:\n",
    "            return cv2.resize(face.image, (size, size))\n",
    "        alignment = get_align_mat( face )\n",
    "        return self.transform( image, alignment, size, 48 )\n",
    "    \n",
    "    def transform( self, image, mat, size, padding=0 ):\n",
    "        mat = mat * (size - 2 * padding)\n",
    "        mat[:,2] += padding\n",
    "        return cv2.warpAffine( image, mat, ( size, size ) )\n",
    "     \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:\n",
    "We will be using a single Encoder and two Decoders to train our model. For the Encoder we use 4 Convolutional layers with activation LeakyReLU and neurons double at each layer, followed by Dense and Flatten. For the Decoder we use 3 Convolutional layers with activation LeakyReLU, followed by another Convolutional layer with Sigmoid activation. We use Adam as the optimizer with learning rate 0.00005 . The loss function used is Mean Absolute Error.\n",
    "![autoencoderworking](images/04.png)\n",
    "\n",
    "### Code for trainer:\n",
    "```python\n",
    "from lib.utils import get_image_paths\n",
    "from lib.cli import FullPaths\n",
    "from plugins.PluginLoader import PluginLoader\n",
    "\n",
    "class TrainingProcessor(object):\n",
    "    arguments = None\n",
    "\n",
    "    def __init__(self, subparser, command, description='default'):\n",
    "        self.parse_arguments(description, subparser, command)\n",
    "\n",
    "    def process_arguments(self, arguments):\n",
    "        self.arguments = arguments\n",
    "        print(\"Model A Directory: {}\".format(self.arguments.input_A))\n",
    "        print(\"Model B Directory: {}\".format(self.arguments.input_B))\n",
    "        print(\"Training data directory: {}\".format(self.arguments.model_dir))\n",
    "\n",
    "        self.process()\n",
    "\n",
    "    def parse_arguments(self, description, subparser, command):\n",
    "        parser = subparser.add_parser(\n",
    "            command,\n",
    "            help=\"This command trains the model for the two faces A and B.\",\n",
    "            description=description\n",
    "        )\n",
    "\n",
    "        parser.add_argument('-A', '--input-A',\n",
    "                            action=FullPaths,\n",
    "                            dest=\"input_A\",\n",
    "                            default=\"input_A\",\n",
    "                            help=\"Input directory. A directory containing training images for face A.\\\n",
    "                             Defaults to 'input'\")\n",
    "        parser.add_argument('-B', '--input-B',\n",
    "                            action=FullPaths,\n",
    "                            dest=\"input_B\",\n",
    "                            default=\"input_B\",\n",
    "                            help=\"Input directory. A directory containing training images for face B.\\\n",
    "                             Defaults to 'input'\")\n",
    "        parser.add_argument('-m', '--model-dir',\n",
    "                            action=FullPaths,\n",
    "                            dest=\"model_dir\",\n",
    "                            default=\"models\",\n",
    "                            help=\"Model directory. This is where the training data will \\\n",
    "                                be stored. Defaults to 'model'\")\n",
    "        parser.add_argument('-p', '--preview',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"preview\",\n",
    "                            default=False,\n",
    "                            help=\"Show preview output. If not specified, write progress \\\n",
    "                            to file.\")\n",
    "        parser.add_argument('-v', '--verbose',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"verbose\",\n",
    "                            default=False,\n",
    "                            help=\"Show verbose output\")\n",
    "        parser.add_argument('-s', '--save-interval',\n",
    "                            type=int,\n",
    "                            dest=\"save_interval\",\n",
    "                            default=100,\n",
    "                            help=\"Sets the number of iterations before saving the model.\")\n",
    "        parser.add_argument('-w', '--write-image',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"write_image\",\n",
    "                            default=False,\n",
    "                            help=\"Writes the training result to a file even on preview mode.\")\n",
    "        parser.add_argument('-t', '--trainer',\n",
    "                            type=str,\n",
    "                            choices=(\"Original\", \"LowMem\"),\n",
    "                            default=\"Original\",\n",
    "                            help=\"Select which trainer to use, LowMem for cards < 2gb.\")\n",
    "        parser.add_argument('-bs', '--batch-size',\n",
    "                            type=int,\n",
    "                            default=64,\n",
    "                            help=\"Batch size, as a power of 2 (64, 128, 256, etc)\")\n",
    "        parser = self.add_optional_arguments(parser)\n",
    "        parser.set_defaults(func=self.process_arguments)\n",
    "\n",
    "    def add_optional_arguments(self, parser):\n",
    "        # Override this for custom arguments\n",
    "        return parser\n",
    "\n",
    "    def process(self):\n",
    "        import threading\n",
    "        self.stop = False\n",
    "        self.save_now = False\n",
    "\n",
    "        thr = threading.Thread(target=self.processThread, args=(), kwargs={})\n",
    "        thr.start()\n",
    "\n",
    "        if self.arguments.preview:\n",
    "            print('Using live preview')\n",
    "            while True:\n",
    "                try:\n",
    "                    for name, image in self.preview_buffer.items():\n",
    "                        cv2.imshow(name, image)\n",
    "\n",
    "                    key = cv2.waitKey(1000)\n",
    "                    if key == ord('\\n') or key == ord('\\r'):\n",
    "                        break\n",
    "                    if key == ord('s'):\n",
    "                        self.save_now = True\n",
    "                except KeyboardInterrupt:\n",
    "                    break\n",
    "        else:\n",
    "            input() \n",
    "\n",
    "        print(\"Exit requested! The trainer will complete its current cycle, save the models and quit (it can take up a couple of seconds depending on your training speed). If you want to kill it now, press Ctrl + c\")\n",
    "        self.stop = True\n",
    "        thr.join() # waits until thread finishes\n",
    "\n",
    "    def processThread(self):\n",
    "        print('Loading data, this may take a while...')\n",
    "        # this is so that you can enter case insensitive values for trainer\n",
    "        trainer = self.arguments.trainer\n",
    "        trainer = trainer if trainer != \"Lowmem\" else \"LowMem\"\n",
    "        model = PluginLoader.get_model(trainer)(self.arguments.model_dir)\n",
    "        model.load(swapped=False)\n",
    "\n",
    "        images_A = get_image_paths(self.arguments.input_A)\n",
    "        images_B = get_image_paths(self.arguments.input_B)\n",
    "        trainer = PluginLoader.get_trainer(trainer)(model,\n",
    "                                                                   images_A,\n",
    "                                                                   images_B,\n",
    "                                                                   batch_size=self.arguments.batch_size)\n",
    "\n",
    "        try:\n",
    "            print('Starting. Press \"Enter\" to stop training and save model')\n",
    "\n",
    "            for epoch in range(0, 1000000):\n",
    "\n",
    "                save_iteration = epoch % self.arguments.save_interval == 0\n",
    "\n",
    "                trainer.train_one_step(epoch, self.show if (save_iteration or self.save_now) else None)\n",
    "\n",
    "                if save_iteration:\n",
    "                    model.save_weights()\n",
    "\n",
    "                if self.stop:\n",
    "                    model.save_weights()\n",
    "                    exit()\n",
    "\n",
    "                if self.save_now:\n",
    "                    model.save_weights()\n",
    "                    self.save_now = False\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            try:\n",
    "                model.save_weights()\n",
    "            except KeyboardInterrupt:\n",
    "                print('Saving model weights has been cancelled!')\n",
    "            exit(0)\n",
    "\n",
    "    preview_buffer = {}\n",
    "\n",
    "    def show(self, image, name=''):\n",
    "        try:\n",
    "            if self.arguments.preview:\n",
    "                self.preview_buffer[name] = image\n",
    "            elif self.arguments.write_image:\n",
    "                cv2.imwrite('_sample_{}.jpg'.format(name), image)\n",
    "        except Exception as e:\n",
    "            print(\"could not preview sample\")\n",
    "            print(e)\n",
    "            ```\n",
    "### Code for AutoEncoder\n",
    "```python\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from lib.ModelAE import ModelAE, TrainerAE\n",
    "from lib.PixelShuffler import PixelShuffler\n",
    "\n",
    "IMAGE_SHAPE = (64, 64, 3)\n",
    "ENCODER_DIM = 1024\n",
    "\n",
    "class Model(ModelAE):\n",
    "    def initModel(self):\n",
    "        optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999)\n",
    "        x = Input(shape=IMAGE_SHAPE)\n",
    "\n",
    "        self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))\n",
    "        self.autoencoder_B = KerasModel(x, self.decoder_B(self.encoder(x)))\n",
    "\n",
    "        self.autoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "        self.autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "\n",
    "    def converter(self, swap):\n",
    "        autoencoder = self.autoencoder_B if not swap else self.autoencoder_A \n",
    "        return lambda img: autoencoder.predict(img)\n",
    "\n",
    "    def conv(self, filters):\n",
    "        def block(x):\n",
    "            x = Conv2D(filters, kernel_size=5, strides=2, padding='same')(x)\n",
    "            x = LeakyReLU(0.1)(x)\n",
    "            return x\n",
    "        return block\n",
    "\n",
    "    def upscale(self, filters):\n",
    "        def block(x):\n",
    "            x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)\n",
    "            x = LeakyReLU(0.1)(x)\n",
    "            x = PixelShuffler()(x)\n",
    "            return x\n",
    "        return block\n",
    "\n",
    "    def Encoder(self):\n",
    "        input_ = Input(shape=IMAGE_SHAPE)\n",
    "        x = input_\n",
    "        x = self.conv(128)(x)\n",
    "        x = self.conv(256)(x)\n",
    "        x = self.conv(512)(x)\n",
    "        x = self.conv(1024)(x)\n",
    "        x = Dense(ENCODER_DIM)(Flatten()(x))\n",
    "        x = Dense(4 * 4 * 1024)(x)\n",
    "        x = Reshape((4, 4, 1024))(x)\n",
    "        x = self.upscale(512)(x)\n",
    "        return KerasModel(input_, x)\n",
    "\n",
    "    def Decoder(self):\n",
    "        input_ = Input(shape=(8, 8, 512))\n",
    "        x = input_\n",
    "        x = self.upscale(256)(x)\n",
    "        x = self.upscale(128)(x)\n",
    "        x = self.upscale(64)(x)\n",
    "        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)\n",
    "        return KerasModel(input_, x)\n",
    "\n",
    "class Trainer(TrainerAE):\n",
    "    \"\"\"Empty inheritance\"\"\"\n",
    "```\n",
    "### Training Progress\n",
    "![Training](images/05.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract frames from Video \n",
    "We use ffmpeg to extract frames from the video\n",
    "\n",
    "### Code for performing Swap on the extracted frames\n",
    "```python\n",
    "import cv2\n",
    "import re\n",
    "from pathlib import Path\n",
    "from lib.cli import DirectoryProcessor, FullPaths\n",
    "from lib.utils import BackgroundGenerator\n",
    "from lib.faces_detect import detect_faces\n",
    "from plugins.PluginLoader import PluginLoader\n",
    "\n",
    "class ConvertImage(DirectoryProcessor):\n",
    "    filename = ''\n",
    "    def create_parser(self, subparser, command, description):\n",
    "        self.parser = subparser.add_parser(\n",
    "            command,\n",
    "            help=\"Convert a source image to a new one with the face swapped.\",\n",
    "            description=description     \n",
    "        )\n",
    "\n",
    "    def add_optional_arguments(self, parser):\n",
    "        parser.add_argument('-m', '--model-dir',\n",
    "                            action=FullPaths,\n",
    "                            dest=\"model_dir\",\n",
    "                            default=\"models\",\n",
    "                            help=\"Model directory. A directory containing the trained model \\\n",
    "                    you wish to process. Defaults to 'models'\")\n",
    "\n",
    "        parser.add_argument('-s', '--swap-model',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"swap_model\",\n",
    "                            default=False,\n",
    "                            help=\"Swap the model. Instead of A -> B, swap B -> A.\")\n",
    "\n",
    "        parser.add_argument('-c', '--converter',\n",
    "                            type=str,\n",
    "                            choices=(\"Masked\", \"Adjust\"), # case sensitive because this is used to load a plugin.\n",
    "                            default=\"Masked\",\n",
    "                            help=\"Converter to use.\")\n",
    "\n",
    "        parser.add_argument('-fr', '--frame-ranges',\n",
    "                            nargs=\"+\",\n",
    "                            type=str,\n",
    "                            help=\"\"\"frame ranges to apply transfer to. eg for frames 10 to 50 and 90 to 100 use --frame-ranges 10-50 90-100.\n",
    "                            Files must have the framenumber as the last number in the name!\"\"\"\n",
    "                            )\n",
    "\n",
    "        parser.add_argument('-d', '--discard-frames',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"discard_frames\",\n",
    "                            default=False,\n",
    "                            help=\"when use with --frame-ranges discards frames that are not processed instead of writing them out unchanged.\"\n",
    "                            )\n",
    "\n",
    "        parser.add_argument('-b', '--blur-size',\n",
    "                            type=int,\n",
    "                            default=2,\n",
    "                            help=\"Blur size. (Masked converter only)\")\n",
    "\n",
    "        parser.add_argument('-S', '--seamless',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"seamless_clone\",\n",
    "                            default=False,\n",
    "                            help=\"Seamless mode. (Masked converter only)\")\n",
    "\n",
    "        parser.add_argument('-M', '--mask-type', \n",
    "        # Based on https://gist.github.com/anonymous/d3815aba83a8f79779451262599b0955\n",
    "                            type=str.lower, #lowercase this, because its just a string later on.\n",
    "                            dest=\"mask_type\",\n",
    "                            choices=[\"rect\", \"facehull\", \"facehullandrect\"],\n",
    "                            default=\"facehullandrect\",\n",
    "                            help=\"Mask to use to replace faces. (Masked converter only)\")\n",
    "\n",
    "        parser.add_argument('-e', '--erosion-kernel-size',\n",
    "                            dest=\"erosion_kernel_size\",\n",
    "                            type=int,\n",
    "                            default=None,\n",
    "                            help=\"Erosion kernel size. (Masked converter only)\")\n",
    "\n",
    "        parser.add_argument('-sm', '--smooth-mask',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"smooth_mask\",\n",
    "                            default=True,\n",
    "                            help=\"Smooth mask (Adjust converter only)\")\n",
    "\n",
    "        parser.add_argument('-aca', '--avg-color-adjust',\n",
    "                            action=\"store_true\",\n",
    "                            dest=\"avg_color_adjust\",\n",
    "                            default=True,\n",
    "                            help=\"Average color adjust. (Adjust converter only)\")\n",
    "\n",
    "        return parser\n",
    "    \n",
    "    def process(self):\n",
    "        # Original model goes with Adjust or Masked converter\n",
    "        # does the LowMem one work with only one?\n",
    "        model_name = \"Original\" # TODO Pass as argument\n",
    "        conv_name = self.arguments.converter\n",
    "        \n",
    "        model = PluginLoader.get_model(model_name)(self.arguments.model_dir)\n",
    "        if not model.load(self.arguments.swap_model):\n",
    "            print('Model Not Found! A valid model must be provided to continue!')\n",
    "            exit(1)\n",
    "\n",
    "        converter = PluginLoader.get_converter(conv_name)(model.converter(False),\n",
    "            blur_size=self.arguments.blur_size,\n",
    "            seamless_clone=self.arguments.seamless_clone,\n",
    "            mask_type=self.arguments.mask_type,\n",
    "            erosion_kernel_size=self.arguments.erosion_kernel_size,\n",
    "            smooth_mask=self.arguments.smooth_mask,\n",
    "            avg_color_adjust=self.arguments.avg_color_adjust\n",
    "        )\n",
    "\n",
    "        batch = BackgroundGenerator(self.prepare_images(), 1)\n",
    "\n",
    "        # frame ranges stuff...\n",
    "        self.frame_ranges = None\n",
    "        # split out the frame ranges and parse out \"min\" and \"max\" values\n",
    "        minmax = {\n",
    "            \"min\": 0, # never any frames less than 0\n",
    "            \"max\": float(\"inf\")\n",
    "        }\n",
    "        if self.arguments.frame_ranges:\n",
    "            self.frame_ranges = [tuple(map(lambda q: minmax[q] if q in minmax.keys() else int(q), v.split(\"-\"))) for v in self.arguments.frame_ranges]\n",
    "\n",
    "        # last number regex. I know regex is hacky, but its reliablyhacky(tm).\n",
    "        self.imageidxre = re.compile(r'(\\d+)(?!.*\\d)')\n",
    "\n",
    "        for item in batch.iterator():\n",
    "            self.convert(converter, item)\n",
    "    \n",
    "    def check_skip(self, filename):\n",
    "        try:\n",
    "            idx = int(self.imageidxre.findall(filename)[0])\n",
    "            return not any(map(lambda b: b[0]<=idx<=b[1], self.frame_ranges))\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def convert(self, converter, item):\n",
    "        try:\n",
    "            (filename, image, faces) = item\n",
    "            \n",
    "            skip = self.check_skip(filename)\n",
    "\n",
    "            if not skip: # process as normal\n",
    "                for idx, face in faces:\n",
    "                    image = converter.patch_image(image, face)\n",
    "\n",
    "            output_file = self.output_dir / Path(filename).name\n",
    "\n",
    "            if self.arguments.discard_frames and skip:\n",
    "                return\n",
    "            cv2.imwrite(str(output_file), image)\n",
    "        except Exception as e:\n",
    "            print('Failed to convert image: {}. Reason: {}'.format(filename, e))\n",
    "\n",
    "    def prepare_images(self):\n",
    "        for filename in self.read_directory():\n",
    "            image = cv2.imread(filename)\n",
    "            yield filename, image, self.get_faces(image)\n",
    "\n",
    "```\n",
    "### Masked Conversion:\n",
    "```python\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "from lib.aligner import get_align_mat\n",
    "\n",
    "class Convert():\n",
    "    def __init__(self, encoder, blur_size=2, seamless_clone=False, mask_type=\"facehullandrect\", erosion_kernel_size=None, **kwargs):\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.erosion_kernel = None\n",
    "        if erosion_kernel_size is not None:\n",
    "            self.erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(erosion_kernel_size,erosion_kernel_size))\n",
    "\n",
    "        self.blur_size = blur_size\n",
    "        self.seamless_clone = seamless_clone\n",
    "        self.mask_type = mask_type.lower() # Choose in 'FaceHullAndRect','FaceHull','Rect'\n",
    "\n",
    "    def patch_image( self, image, face_detected ):\n",
    "        size = 64\n",
    "        image_size = image.shape[1], image.shape[0]\n",
    "\n",
    "        mat = numpy.array(get_align_mat(face_detected)).reshape(2,3) * size\n",
    "\n",
    "        new_face = self.get_new_face(image,mat,size)\n",
    "\n",
    "        image_mask = self.get_image_mask( image, new_face, face_detected, mat, image_size )\n",
    "\n",
    "        return self.apply_new_face(image, new_face, image_mask, mat, image_size, size)\n",
    "\n",
    "    def apply_new_face(self, image, new_face, image_mask, mat, image_size, size):\n",
    "        base_image = numpy.copy( image )\n",
    "        new_image = numpy.copy( image )\n",
    "\n",
    "        cv2.warpAffine( new_face, mat, image_size, new_image, cv2.WARP_INVERSE_MAP, cv2.BORDER_TRANSPARENT )\n",
    "\n",
    "        outImage = None\n",
    "        if self.seamless_clone:\n",
    "            masky,maskx = cv2.transform( numpy.array([ size/2,size/2 ]).reshape(1,1,2) ,cv2.invertAffineTransform(mat) ).reshape(2).astype(int)\n",
    "            outimage = cv2.seamlessClone(new_image.astype(numpy.uint8),base_image.astype(numpy.uint8),(image_mask*255).astype(numpy.uint8),(masky,maskx) , cv2.NORMAL_CLONE )\n",
    "        else:\n",
    "            foreground = cv2.multiply(image_mask, new_image.astype(float))\n",
    "            background = cv2.multiply(1.0 - image_mask, base_image.astype(float))\n",
    "            outimage = cv2.add(foreground, background)\n",
    "\n",
    "        return outimage\n",
    "\n",
    "    def get_new_face(self, image, mat, size):\n",
    "        face = cv2.warpAffine( image, mat, (size,size) )\n",
    "        face = numpy.expand_dims( face, 0 )\n",
    "        new_face = self.encoder( face / 255.0 )[0]\n",
    "\n",
    "        return numpy.clip( new_face * 255, 0, 255 ).astype( image.dtype )\n",
    "\n",
    "    def get_image_mask(self, image, new_face, face_detected, mat, image_size):\n",
    "\n",
    "        face_mask = numpy.zeros(image.shape,dtype=float)\n",
    "        if 'rect' in self.mask_type:\n",
    "            face_src = numpy.ones(new_face.shape,dtype=float)\n",
    "            cv2.warpAffine( face_src, mat, image_size, face_mask, cv2.WARP_INVERSE_MAP, cv2.BORDER_TRANSPARENT )\n",
    "\n",
    "        hull_mask = numpy.zeros(image.shape,dtype=float)\n",
    "        if 'hull' in self.mask_type:\n",
    "            hull = cv2.convexHull( numpy.array( face_detected.landmarksAsXY() ).reshape((-1,2)).astype(int) ).flatten().reshape( (-1,2) )\n",
    "            cv2.fillConvexPoly( hull_mask,hull,(1,1,1) )\n",
    "\n",
    "        if self.mask_type == 'rect':\n",
    "            image_mask = face_mask\n",
    "        elif self.mask_type == 'faceHull':\n",
    "            image_mask = hull_mask\n",
    "        else:\n",
    "            image_mask = ((face_mask*hull_mask))\n",
    "\n",
    "\n",
    "        if self.erosion_kernel is not None:\n",
    "            image_mask = cv2.erode(image_mask,self.erosion_kernel,iterations = 1)\n",
    "\n",
    "        if self.blur_size!=0:\n",
    "            image_mask = cv2.blur(image_mask,(self.blur_size,self.blur_size))\n",
    "\n",
    "        return image_mask\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align='center'> Image before conversion</h3>\n",
    "\n",
    "![stark_before.png](images/06.png)\n",
    "\n",
    "<h3 align='center'> Image after conversion</h3>\n",
    "\n",
    "![stark_after.png](images/07.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align='center'> Model Results </h3>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Error for A</th>\n",
    "    <th>Error for B</th>\n",
    "    <th>Batch Size</th>\n",
    "    <th>Epoch</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.05862</td>\n",
    "    <td>0.07512</td>\n",
    "    <td>64</td>\n",
    "    <td>1000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.05322</td>\n",
    "    <td>0.07353</td>\n",
    "    <td>64</td>\n",
    "    <td>1500</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.05226</td>\n",
    "    <td>0.07171</td>\n",
    "    <td>128</td>\n",
    "    <td>1000</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.04723</td>\n",
    "    <td>0.06410</td>\n",
    "    <td>128</td>\n",
    "    <td>1500</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Since the error rate was still 0.04723, we couldn’t obtain significant clarity.\n",
    "But a few thoughts to improve this model would be to increase the dataset, more CNN layers, using GAN’s. Adding a bit of noise in the image dataset and choosing high resolution images may be viable as well.\n",
    "\n",
    "#### Github link for the project\n",
    "https://github.com/sushrutt12/faceswappingtechnique\n",
    "\n",
    "#### Link for the video\n",
    "https://youtu.be/FZXQtbHG6-o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "The text in the document by Sushrut Tadwalkar is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
